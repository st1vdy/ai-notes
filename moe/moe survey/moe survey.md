# A Survey on Mixture of Experts

## Abstract

大型语言模型（LLMs）在从自然语言处理到计算机视觉等多个领域取得了前所未有的进展。LLM的强大能力源于其庞大的模型规模、广泛且多样化的数据集，以及在训练过程中所利用的大量计算能力，这些因素共同促成了LLM的涌现能力（例如，语境学习），而这些能力在小规模模型中并不具备。在这一背景下，专家混合（MoE）作为一种有效的方法，能够在最小的计算开销下大幅扩展模型容量，受到了学术界和工业界的广泛关注。尽管其日益普及，但关于MoE的文献缺乏系统和全面的综述。本文旨在弥补这一空白，成为研究人员深入了解MoE细节的重要资源。我们首先简要介绍MoE层的结构，然后提出一种新的MoE分类法。接下来，我们概述了各种MoE模型的核心设计，包括算法和系统方面的内容，同时提供了可用的开源实现、超参数配置和实证评估的汇总。此外，我们还阐明了MoE在实际中的多种应用，并勾画了未来研究的潜在方向。为了便于持续更新和分享MoE研究的最新进展，我们建立了一个资源库，网址为：https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts。